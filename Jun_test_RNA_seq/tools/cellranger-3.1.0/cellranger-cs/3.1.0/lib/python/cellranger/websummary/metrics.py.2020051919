"""
Tools for managing metrics with metadata attached.

Copyright (c) 2018 10X Genomics, Inc. All rights reserved.

Copied from cellranger-atac (commit 5ccda578ed71e24289c6ea9bb6dec4d9de5d11dc)
"""

from __future__ import division
import pandas as pd
import os
import martian
from cellranger.vdj.utils import load_chains_from_chain_type
from cellranger.rna.library import add_multi_prefix, add_species_prefix

VALID_THRESHOLD = "VALID"
WARNING_THRESHOLD = "WARN"
ERROR_THRESHOLD = "ERROR"


def load_metric_data(metric_csv_file):
    """
    Loads the metric metadata as a dictionary by parsing the input csv file.

    Entries can point to another entry. For example 'alert_warn_detail_cs' field
    could just say 'alert_warn_detail', meaning we want to use the content in
    'alert_warn_detail' for 'alert_warn_detail_cs' as well. Only a single level of
    redirection is supported. i.e You should not have A -> B -> C, instead use A->C
    and B->C. No circular references too (i.e A->B and B->A). The code does not
    explicitly check for these, but these simple rules should be followed when building
    the metrics csv.
    """
    data = pd.read_csv(metric_csv_file, index_col=0, comment="#")
    # Makes empty values None instead of NaN
    data = data.where(pd.notnull(data), None)
    # Code to resolve single level of redirections
    headers = set(data.keys().tolist())
    for i, row in data.iterrows():
        for k, v in row.iteritems():
            if v in headers:
                data.at[i, k] = row[v]
    # Return the data as a dictionary
    metric_data = {}
    for key in data.index.values:
        if metric_data.has_key(key):
            raise IOError("Metrics file cannot contain duplicated keys")
        metric_data[key] = data.loc[key]
    return metric_data


class MetricAnnotations:
    def __init__(self):
        """Load in metric information from the associated csv file.
        """
        file_path = os.path.join(os.path.dirname(
            os.path.abspath(__file__)), 'metrics.csv')
        self.metric_data = load_metric_data(file_path)
        self.source_file = file_path

    def gen_metric(self, key, value, species=None, is_barnyard=False, debug=True, is_cumulative=False):
        """Returns a single metric object for the given key and value.  Alerts are raised when metric falls outside
        normal range, which depends on debug status.

        if it's barnyard sample, add species suffix to name and alert_name.
        """
        metric_info = self.metric_data[key]
        name = metric_info.full_name
        alert_name_suffix = ''
        if species is not None and species:
            key = '{species}_{key}'.format(**locals())
            name += ' ({species})'.format(**locals()) if is_barnyard else ''
            alert_name_suffix += ' ({species})'.format(**
                                                       locals()) if is_barnyard else ''

        # alarm ranges are dependent on debug, which indicates internal or customer-facing.
        alert_name_map = {
            WARNING_THRESHOLD: metric_info.alert_warn_name + alert_name_suffix if metric_info.alert_warn_name else None,
            ERROR_THRESHOLD: metric_info.alert_error_name + alert_name_suffix if metric_info.alert_error_name else None,
        }
        # This appears under detail when an alert is raised
        alert_detail_map = {
            WARNING_THRESHOLD: metric_info.alert_warn_detail if debug else metric_info.alert_warn_detail_cs,
            ERROR_THRESHOLD: metric_info.alert_error_detail if debug else metric_info.alert_error_detail_cs,
        }
        acceptable = (
            metric_info.acceptable if debug else metric_info.acceptable_cs)
        targeted = (metric_info.targeted if debug else metric_info.targeted_cs)

        kwargs = {
            'alert_detail_map': alert_detail_map,
            'acceptable': acceptable,
            'targeted': targeted,
            'evaluate_type': metric_info.evaluate_type,
            'format_type': metric_info.format_type,
            'category': metric_info.category,
            'alert_name_map': alert_name_map,
            'is_barnyard': is_barnyard,
            'is_cumulative': is_cumulative
        }
        return Metric(key, name, value, metric_info, **kwargs)

    def compile_summary_metrics(self, value_dict, keys=None, species_list=None):
        """Processes a metrics dictionary and select summary metrics based on 2nd column in metrics.csv
        for keys provided or all registered keys in metrics.csv"""
        keylist = self.metric_data.keys() if keys is None else keys
        is_barnyard = len(species_list) > 1

        output = {}
        for key in keylist:
            if key in self.metric_data:
                metric_info = self.metric_data[key]
                # if it is a summary metric
                if metric_info.summary:
                    # do species specific processing
                    if metric_info.is_species_specific:
                        if species_list is None or not species_list:
                            raise ValueError(
                                'Must provide a species list for species-specific metrics')
                        for species in species_list:
                            # key_suffix = "" if len(species_list) == 1 else "_{}".format(species)
                            # subkey = '{key}{key_suffix}'.format(**locals())
                            subkey = add_species_prefix(species, key)
                            if subkey in value_dict:
                                output.update({subkey: value_dict[subkey]})
                            else:
                                martian.log_info('{} not found in metrics'.format(key))

                        if is_barnyard and metric_info.include_cumulative:
                            subkey = add_multi_prefix(key)
                            if subkey in value_dict:
                                output.update({subkey: value_dict[subkey]})
                            else:
                                martian.log_info(
                                    '{} not found in metrics'.format(subkey))

                    else:
                        if key in value_dict:
                            output.update({key: value_dict[key]})
                        else:
                            martian.log_info(
                                '{} not found in metrics'.format(key))
            else:
                martian.log_info(
                    '{} not found in registered metrics'.format(key))
        return output

    def gen_metric_helptext(self, keys):
        """Processes a metrics dictionary and generates helptext for keys if present in metrics.csv"""
        output = []
        for key in keys:
            if key in self.metric_data:
                metric_info = self.metric_data[key]
                if metric_info.help_description is not None:
                    full_name = metric_info.full_name
                    output += [[full_name, [metric_info.help_description]]]
            else:
                martian.log_info(
                    '{} not found in registered metrics'.format(key))
        return output

    def gen_metric_list(self, value_dict, keys, species_list=None, debug=True):
        """Returns a list of metric objects for the provided keys, using the value dictionary to get values for
        each metric.  When metrics are species-specific, a list of species is required.  Alerts are raised when
        metrics fall outside normal ranges, which depend on debug status."""
        output = []
        is_barnyard = len(species_list) > 1

        for key in keys:
            metric_info = self.metric_data[key]
            if metric_info.is_species_specific:
                if species_list is None or not species_list:
                    raise ValueError(
                        'Must provide a species list for species-specific metrics')

                if is_barnyard and metric_info.include_cumulative:
                    subkey = add_multi_prefix(key)
                    if subkey in value_dict:
                        output.append(self.gen_metric(key, value_dict[subkey], debug=debug, is_barnyard=is_barnyard, is_cumulative=True))
                    else:
                        martian.log_info('{} not found in metrics'.format(subkey))
                        
                for species in species_list:
                    # key_prefix = "" if len(species_list) == 1 else "{}_".format(species)
                    subkey = add_species_prefix(species, key)
                    if subkey in value_dict:
                        output.append(self.gen_metric(
                            key, value_dict[subkey], species, is_barnyard, debug, is_cumulative=False))
                    else:
                        martian.log_info(
                            '{} not found in metrics'.format(subkey))
            else:
                if key in value_dict:
                    output.append(self.gen_metric(
                        key, value_dict[key], debug=debug))
                else:
                    martian.log_info('{} not found in metrics'.format(key))

        return output


class Metric:
    """Contains metadata about a single metric along with methods for evaluating its value with respect to that
    metadata.
    """

    def __init__(self, key, name, value, parent_metric_info,
                 alert_detail_map=None, acceptable=None, targeted=None, evaluate_type=None,
                 format_type=None, category=None, alert_name_map=None, is_barnyard=False, is_cumulative=False):
        """
        :param key: Metric identifier
        :param name: Full Metric Name
        :param value: The value of the metric
        :param parent_metric_info: Information from the row in the metrics.csv relevant to this metric.
        :param alert_detail_map: Details text for both warning and error alerts
        :param acceptable: Value to determines the error level
        :param targeted: Value to determine the warning level
        :param evaluate_type: lt/gt/range/exists/is_true/is_false/None
        :param format_type: int/percentage/flt/float
        :param category: A category name this metric belongs to (e.g. Mapping)
        :param alert_name_map: Alert text for both warning and error alerts
        :param is_barnyard: True if this metric was generated from a barnyard experiment
        :param is_cumulative: True if this metric is accumulated over multiple genomes
        """
        self.key = key
        self.name = name
        self.value = value
        self.alert_detail_map = alert_detail_map
        self.is_barnyard = is_barnyard
        self.is_cumulative = is_cumulative
        self.parent_metric_info = parent_metric_info
        try:
            self.acceptable = float(acceptable)
        except (ValueError, TypeError):
            self.acceptable = acceptable
        try:
            self.targeted = float(targeted)
        except (ValueError, TypeError):
            self.targeted = targeted
        self.alert_name_map = alert_name_map

        function_map = {None: lambda x, y: True,
                        'lt': self._less_than,
                        'gt': self._greater_than,
                        'range': self._in_range,
                        'exists': self._exists,
                        'is_true': self._is_true,
                        'is_false': self._is_false}
        if evaluate_type not in function_map:
            raise ValueError(
                'Unknown evaluation type: {}'.format(evaluate_type))
        self.evaluate_type = evaluate_type
        self.evaluation_function = function_map[evaluate_type]

        if format_type is None:
            format_type = 'flat'
        if format_type not in ['flat', 'float', 'percentage', 'int']:
            raise ValueError('Unknown format type: {}'.format(format_type))
        self.format_type = format_type

        if category is None:
            category = 'General'
        self.category = category

    def gen_metric_dict(self):
        threshold = self.threshold_type
        translate_dict = {
            VALID_THRESHOLD: 'pass',
            WARNING_THRESHOLD: 'warn',
            ERROR_THRESHOLD: 'error',
        }
        return {
            "threshold": translate_dict[threshold],
            "metric": self.value_string,
            "name": self.name,
        }

    @property
    def alarm_dict(self):
        threshold = self.threshold_type
        if threshold == VALID_THRESHOLD:
            return {}

        return {
            "raw_value": self.value,
            "formatted_value": self.value_string,
            "raised": True,
            "parent": self.key,
            "title": self.alert_name_map[threshold],
            "message": self.alert_detail_map[threshold],
            "level": threshold,
            "test": "",
            "id": self.key
        }

    # Evaluation methods
    @staticmethod
    def _less_than(value, target):
        return value < target

    @staticmethod
    def _greater_than(value, target):
        return value > target

    @staticmethod
    def _in_range(value, target):
        return (value > target[0]) and (value < target[1])

    @staticmethod
    def _exists(value, target):
        return value is not None

    @staticmethod
    def _is_true(value, target):
        return value is True

    @staticmethod
    def _is_false(value, target):
        return value is False

    @property
    def threshold_type(self):
        if self.targeted is None and self.acceptable is None:
            return VALID_THRESHOLD
        elif self.targeted is None:
            # Only acceptable - error if we don't meet them.
            if self.evaluation_function(self.value, self.acceptable):
                return VALID_THRESHOLD
            return ERROR_THRESHOLD
        elif self.acceptable is None:
            # Only targets - warn if we don't meet them.
            if self.evaluation_function(self.value, self.targeted):
                return VALID_THRESHOLD
            return WARNING_THRESHOLD
        else:
            # Both set - error/warn depending on which we meet.
            if self.evaluation_function(self.value, self.targeted):
                return VALID_THRESHOLD
            elif self.evaluation_function(self.value, self.acceptable):
                return WARNING_THRESHOLD
            else:
                return ERROR_THRESHOLD

    @property
    def color(self):
        if self.targeted is None and self.acceptable is None:
            # Meaningless, return grey hex code
            return 'BEBEBE'
        threshold = self.threshold_type
        if threshold == VALID_THRESHOLD:
            return "B4FFB4"
        elif threshold == WARNING_THRESHOLD:
            return "FFFFB4"
        else:
            return "FFB4B4"

    @property
    def acceptable_string(self):
        return self._format_target_value(self.acceptable)

    @property
    def targeted_string(self):
        return self._format_target_value(self.targeted)

    @property
    def value_string(self):
        return self._format_value(self.value)

    def _format_target_value(self, value):
        if value is None:
            return ''

        if self.evaluate_type == "exists":
            return 'Exists'
        if self.evaluate_type == "is_true":
            return 'True'
        if self.evaluate_type == "is_false":
            return 'False'

        if self.evaluate_type == "lt":
            return '< {}'.format(self._format_value(value))
        if self.evaluate_type == "gt":
            return '> {}'.format(self._format_value(value))
        if self.evaluate_type == "range":
            return '{} - {}'.format(self._format_value(value[0]), self._format_value(value[1]))
        raise AssertionError("unreachable - invalid evaluate_type")

    def _format_value(self, value):
        if value is None or value == "NaN":
            return 'None'

        if self.format_type == 'flat':
            return '{}'.format(value)
        elif self.format_type == 'float':
            return '{:,.2f}'.format(value)
        elif self.format_type == 'percentage':
            return '{:.1%}'.format(value)
        elif self.format_type == 'int':
            return '{:,.0f}'.format(value)
        raise AssertionError("unreachable - invalid format_type")


class VDJMetricAnnotations:
    def __init__(self):
        """
        Load metric information from the associated csv file.
        """
        ############ TODO: CHANGE THIS ###########################
        file_path = os.path.join(os.path.dirname(
            os.path.abspath(__file__)), 'vdj_metrics.csv')
        self.metric_data = load_metric_data(file_path)

    def gen_metric(self, key, value, debug=False, chain=None):
        """
        Returns a single metric object for the given key and value.  Alerts are raised when metric falls outside
        normal range, which depends on debug status.
        """
        metric_info = self.metric_data[key]
        name = metric_info.full_name

        key = key.format(chain=chain)
        name = name.format(chain=chain)

        alert_name_map = {
            WARNING_THRESHOLD: metric_info.alert_warn_name.format(chain=chain) if metric_info.alert_warn_name else None,
            ERROR_THRESHOLD: metric_info.alert_error_name.format(chain=chain) if metric_info.alert_error_name else None,
        }

        # alarm ranges are dependent on debug, which indicates internal or customer-facing.
        # This appears under detail when an alert is raised
        alert_warn_detail = metric_info.alert_warn_detail if debug else metric_info.alert_warn_detail_cs
        alert_error_detail = metric_info.alert_error_detail if debug else metric_info.alert_error_detail_cs
        alert_detail_map = {
            WARNING_THRESHOLD: alert_warn_detail.format(chain=chain) if alert_warn_detail else None,
            ERROR_THRESHOLD: alert_error_detail.format(chain=chain) if alert_error_detail else None,
        }
        acceptable = (
            metric_info.acceptable if debug else metric_info.acceptable_cs)
        targeted = (metric_info.targeted if debug else metric_info.targeted_cs)

        kwargs = {
            'alert_detail_map': alert_detail_map,
            'acceptable': acceptable,
            'targeted': targeted,
            'evaluate_type': metric_info.evaluate_type,
            'format_type': metric_info.format_type,
            'category': metric_info.category,
            'alert_name_map': alert_name_map,
        }
        return Metric(key, name, value, metric_info, **kwargs)

    def gen_metric_list(self, value_dict, keys, debug=False, chain_type=None):
        """
        Returns a list of metric objects for the provided keys, using the value dictionary to get values for
        each metric. Alerts are raised when metrics fall outside normal ranges, which depend on debug status.
        """
        output = []
        for key in keys:
            metric_info = self.metric_data[key]
            if metric_info.is_chain_specific:
                assert chain_type is not None, "Got no chain type for {}".format(
                    key)
                for chain in load_chains_from_chain_type(chain_type):
                    full_key = key.format(chain=chain)
                    if full_key in value_dict:
                        output.append(self.gen_metric(
                            key, value_dict[full_key], debug=debug, chain=chain))
                    else:
                        print '{} not found in metrics'.format(full_key)
                        # martian.log_info('{} not found in metrics'.format(full_key))
            else:
                if key in value_dict:
                    output.append(self.gen_metric(
                        key, value_dict[key], debug=debug))
                else:
                    print '{} not found in metrics'.format(key)
                    # martian.log_info('{} not found in metrics'.format(key))
        return output

    def gen_metric_helptext(self, keys, chain_type=None):
        """Processes a metrics dictionary and generates helptext for keys if present in the metrics csv"""
        output = []
        for key in keys:
            if key in self.metric_data:
                metric_info = self.metric_data[key]
                if metric_info.is_chain_specific:
                    assert chain_type is not None, "Got no chain type for {}".format(
                        key)
                    for chain in load_chains_from_chain_type(chain_type):
                        full_name = metric_info.full_name.format(chain=chain)
                        output += [[full_name,
                                    [metric_info.help_description.format(chain=chain)]]]
                else:
                    if metric_info.help_description is not None:
                        full_name = metric_info.full_name
                        output += [[full_name, [metric_info.help_description]]]
            else:
                print '{} not found in registered metrics'.format(key)
                # martian.log_info('{} not found in registered metrics'.format(key))
        return output
